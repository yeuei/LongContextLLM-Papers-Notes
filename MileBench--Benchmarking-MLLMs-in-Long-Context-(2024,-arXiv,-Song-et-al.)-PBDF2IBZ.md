---
tags: []
parent: 'MileBench: Benchmarking MLLMs in Long Context'
collections:
    - 'Long context LLM'
$version: 5941
$libraryID: 1
$itemKey: PBDF2IBZ

---
# MileBench: Benchmarking MLLMs in Long Context (2024, arXiv, Song et al.)

***

| <!-- --> |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **<span style="color: rgb(60, 90, 204)"><span style="background-color: rgb(246, 248, 250)">Author: </span></span>**<span style="background-color: rgb(246, 248, 250)">Dingjie Song; Shunian Chen; Guiming Hardy Chen; Fei Yu; Xiang Wan; Benyou Wang</span>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| **<span style="color: rgb(60, 90, 204)"><span style="background-color: rgb(236, 243, 250)">Source: </span></span>**<span style="background-color: rgb(236, 243, 250)">arXiv</span>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **<span style="color: rgb(60, 90, 204)"><span style="background-color: rgb(246, 248, 250)">DOI: </span></span>**<span style="background-color: rgb(246, 248, 250)"><a href="https://doi.org/10.48550/arXiv.2404.18532" rel="noopener noreferrer nofollow">10.48550/arXiv.2404.18532</a></span>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| **<span style="color: rgb(60, 90, 204)"><span style="background-color: rgb(236, 243, 250)">Date: </span></span>**<span style="background-color: rgb(236, 243, 250)">2024-05-15</span>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| **<span style="color: rgb(60, 90, 204)"><span style="background-color: rgb(246, 248, 250)">Full Text: </span></span>**<span style="background-color: rgb(246, 248, 250)"><a href="zotero://open-pdf/0_DGHN9QHG" rel="noopener noreferrer nofollow">Song 等 - 2024 - MileBench Benchmarking MLLMs in Long Context.pdf</a></span>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| **<span style="color: rgb(60, 90, 204)"><span style="background-color: rgb(236, 243, 250)">Abstract: </span></span>***<span style="background-color: rgb(236, 243, 250)">Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope. Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs. To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios. Our experimental results, obtained from testing 22 models, revealed that while the closed-source GPT-4o outperforms others, most open-source MLLMs struggle in long-context situations. Interestingly, the performance gap tends to widen with an increase in the number of images. We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images.</span>* |


***

## <span style="color: rgb(190, 27, 65)">🌏背景&#x26;目前的挑战：</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22INT49RGG%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B245.626%2C251.962%2C468.13%2C261.546%5D%2C%5B143.865%2C241.003%2C468.33%2C250.587%5D%2C%5B143.865%2C230.044%2C345.09%2C239.628%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=1&#x26;annotation=INT49RGG"><span style="background-color: #ff666680">“Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 1</a></span>)</span>\
**现有的基准通常关注单张图像和短文本样本，而在评估多图像任务时，它们要么限制图像数量，要么专注于特定任务。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%228H3SFK8Y%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B166.662%2C334.012%2C505.651%2C343.596%5D%2C%5B107.691%2C323.053%2C504.004%2C332.637%5D%2C%5B108%2C312.094%2C503.998%2C321.678%5D%2C%5B108%2C301.135%2C504.001%2C310.719%5D%2C%5B107.671%2C290.176%2C503.995%2C299.76%5D%2C%5B108%2C279.217%2C447.944%2C288.801%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=2&#x26;annotation=8H3SFK8Y"><span style="background-color: #ff666680">“existing benchmarks primarily focus on single-image and short-text samples (Liu et al., 2023a; Fu et al., 2023; Liu et al., 2023c; Li et al., 2023b), thereby failing to fully capture the complexity and diversity of real-world scenarios. While some benchmarks evaluate multi-image tasks, they either have limited number of images provided per sample (e.g., SEED-Bench-2 (Li et al., 2023a), DEMON (Li et al., 2023c)) or only include time-series captioning tasks (e.g., Mementos (Wang et al., 2024)), as shown in Figure 2.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 2</a></span>)</span>\
**现有的基准测试主要关注单图像和短文本样本（Liu等，2023a；Fu等，2023；Liu等，2023c；Li等，2023b），因此未能充分捕捉真实世界场景的复杂性和多样性。尽管某些基准测试评估多图像任务，但它们提供的每个样本的图像数量有限（例如，SEED-Bench-2（Li等，2023a），DEMON（Li等，2023c）），或者仅包括时间序列描述任务（例如，Mementos（Wang等，2024）），如图2所示。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22J3FF2JAQ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B452.273%2C279.217%2C505.247%2C288.801%5D%2C%5B108%2C268.258%2C503.998%2C277.842%5D%2C%5B108%2C257.299%2C316.918%2C266.883%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=2&#x26;annotation=J3FF2JAQ"><span style="background-color: #ff666680">“In addition, this omission could potentially neglect the hallucination issue that MLLMs might exhibit in long-context situations (Huang et al., 2023).”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 2</a></span>)</span>\
**此外，这一遗漏可能会忽视多模态大语言模型在长上下文场景中可能表现出的幻觉问题（Huang 等, 2023）。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22TE3HKTY7%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B205.918%2C235.381%2C503.997%2C244.965%5D%2C%5B108%2C224.422%2C164.259%2C234.006%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=2&#x26;annotation=TE3HKTY7"><span style="background-color: #ff666680">“long-context and multi-image task demands prevalent in real-world applications.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 2</a></span>)</span>\
**现实世界应用中普遍存在的长上下文和多图像任务需求。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22MQYE5HL7%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B108%2C562.216%2C505.654%2C571.8%5D%2C%5B108%2C551.257%2C429.257%2C560.841%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=3&#x26;annotation=MQYE5HL7"><span style="background-color: #ff666680">“Beyond training on single-image-text pairs, recent developments in MLLMs are also oriented towards handling multiple and interleaved image-text sequences”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 3</a></span>)</span>\
**除基于单图像-文本对的训练外，最近关于多模态大型语言模型（MLLMs）的研究也倾向于处理多图像-文本序列以及交错图像-文本序列。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%226FJ9694D%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B339.452%2C507.421%2C504.294%2C517.005%5D%2C%5B108%2C496.462%2C390.961%2C506.046%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=3&#x26;annotation=6FJ9694D"><span style="background-color: #ff666680">“However, there remains a notable gap in open-source MLLMs capable of long-context comprehension.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 3</a></span>)</span>\
**然而，开源的能够理解长上下文的多语言大模型（MLLMs）仍然存在显著的空白。**

## <span style="color: rgb(190, 27, 65)">🔬论文提出的方法：</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22NH7KP26Z%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B282.66%2C208.126%2C469.789%2C217.71%5D%2C%5B143.865%2C197.167%2C445.702%2C206.751%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=1&#x26;annotation=NH7KP26Z"><span style="background-color: #5fb23680">“MILEBENCH, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 1</a></span>)</span>\
**MILEBENCH，一个旨在测试多模态长上下文能力的开创性基准测试，用于评估多模态大语言模型 (MLLMs)。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22B8EU6KEQ%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B390.229%2C175.249%2C468.13%2C184.833%5D%2C%5B143.865%2C164.29%2C468.13%2C173.874%5D%2C%5B143.865%2C153.331%2C468.13%2C162.915%5D%2C%5B143.865%2C142.372%2C283.438%2C151.956%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=1&#x26;annotation=B8EU6KEQ"><span style="background-color: #5fb23680">“We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs’ long-context adaptation capacity and their ability to complete tasks in long-context scenarios.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 1</a></span>)</span>\
**我们建立了两个不同的评估集：诊断集和现实集，以系统地评估MLLMs在长上下文适应能力以及其在长上下文场景完成任务的能力。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%222N3KXP7S%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B180.148%2C172.861%2C504.002%2C182.445%5D%2C%5B108%2C161.902%2C503.997%2C171.486%5D%2C%5B108%2C150.943%2C503.997%2C160.527%5D%2C%5B108%2C139.984%2C428.382%2C149.568%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=2&#x26;annotation=2N3KXP7S"><span style="background-color: #5fb23680">“diagnostic evaluation and realistic evaluation. The former explores the long-context recall abilities of MLLMs, using needle-in-a-haystack and image retrieval tasks, while the latter stress-tests the model in a manner akin to real-world conditions using both temporal multi-image tasks and semantic multi-image tasks.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 2</a></span>)</span>\
**诊断评估和现实评估。前者通过“大海捞针”和图像检索任务来探索多模态大语言模型（MLLMs）的长时记忆回溯能力，而后者通过时间性多图像任务和语义性多图像任务，以类似真实世界的条件对模型进行压力测试。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22BZJTJJYM%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B180.057%2C318.224%2C503.999%2C327.808%5D%2C%5B108%2C307.265%2C503.997%2C316.849%5D%2C%5B108%2C296.306%2C186.376%2C305.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=3&#x26;annotation=BZJTJJYM"><span style="background-color: #5fb23680">“MILEBENCH is the first comprehensive benchmark that evaluates MLLMs across both multi-image and long-context dimensions, catering to a broader spectrum of general scenarios.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 3</a></span>)</span>\
**MILEBENCH 是首个综合性基准，能够跨多图像和长上下文维度评估 MLLM，适用于更广泛的通用场景。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22PNGQ3TVT%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B107.502%2C196.907%2C504.004%2C206.491%5D%2C%5B108%2C185.948%2C326.948%2C195.532%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%226%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=6&#x26;annotation=PNGQ3TVT"><span style="background-color: #5fb23680">“We have established a robust data collection process and meticulous review procedures to maintain the integrity and quality of our datasets.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 6</a></span>)</span>\
**我们已建立了完善的数据收集流程和严谨的审查程序，以维护数据集的完整性和质量。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%228MBFWVTL%22%2C%22color%22%3A%22%235fb236%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B488.984%2C321.302%2C504.196%2C330.886%5D%2C%5B108%2C310.343%2C503.996%2C319.927%5D%2C%5B108%2C299.384%2C429.971%2C308.968%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=7&#x26;annotation=8MBFWVTL"><span style="background-color: #5fb23680">“For open-ended generation tasks, the popular n-gram-based metric ROUGE-L is adopted, and accuracy is the metric for multiple-choice and needle-in-a-haystack tasks.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 7</a></span>)</span>\
**对于开放式生成任务，采用流行的基于n元模型的指标ROUGE-L，而对于多项选择题和大海捞针任务，则使用准确率作为指标。**

## <span style="color: rgb(190, 27, 65)">📜论文的结论：</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22VVB5QZJM%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B143.865%2C109.496%2C468.139%2C119.08%5D%2C%5B143.865%2C98.537%2C468.13%2C108.121%5D%2C%5B143.865%2C87.578%2C468.135%2C97.162%5D%2C%5B143.865%2C76.619%2C305.777%2C86.203%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=1&#x26;annotation=VVB5QZJM"><span style="background-color: #2ea8e580">“Interestingly, the performance gap tends to widen with an increase in the number of images. We strongly encourage an intensification of research efforts towards enhancing MLLMs’ long-context capabilities, especially in scenarios involving multiple images.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 1</a></span>)</span>\
**随着图像数量的增加，性能差距往往会扩大。我们强烈建议加大研究力度，着重提升多模态大语言模型（MLLMs）处理长上下文的能力，特别是在涉及多张图像的场景中。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22BAB4R9TH%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B107.502%2C239.175%2C505.241%2C248.759%5D%2C%5B107.671%2C228.216%2C503.999%2C237.8%5D%2C%5B108%2C217.257%2C503.997%2C226.841%5D%2C%5B108%2C206.298%2C503.997%2C215.882%5D%2C%5B108%2C195.339%2C333.935%2C204.923%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=7&#x26;annotation=BAB4R9TH"><span style="background-color: #2ea8e580">“We present the results of our experiments in Table 2 and summarize our findings as follows: (1) Closed-source MLLMs outperform open-source MLLMs in multimodal long-context tasks to date, particularly in diagnostic evaluation of long-context adaptability where the gap between closed-source MLLMs (average: 79.2%, max: 99.4%) and open-source MLLMs (average: 10.1%, max: 37.2%) is significant.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 7</a></span>)</span>\
**(1) 闭源的多模态长文本模型 (MLLMs)在多模态长上下文任务中优于开源的多模态长文本模型(MLLMs),特别是在 长上下文适应性的诊断评估中,闭源的多模态长文本模型(平均:79.2%,最高:99.4%)与 开源的多模态长文本模型(平均:10.1%,最高:37.2%)之间的差距显著。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22Q327BGMF%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B398.162%2C184.38%2C504.002%2C193.964%5D%2C%5B108%2C173.422%2C410.518%2C183.006%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=7&#x26;annotation=Q327BGMF"><span style="background-color: #2ea8e580">“(2) Open-source image models generally perform better than open-source video models.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 7</a></span>)</span>\
**开源图像模型通常比开源视频模型表现更好。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22K5ZWW27P%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B402.657%2C422.276%2C504.004%2C431.86%5D%2C%5B108%2C411.317%2C505.245%2C420.901%5D%2C%5B107.582%2C400.359%2C505.243%2C409.943%5D%2C%5B108%2C389.4%2C504.001%2C399.013%5D%2C%5B108%2C378.441%2C259.792%2C388.025%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%228%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=8&#x26;annotation=K5ZWW27P"><span style="background-color: #2ea8e580">“The ability to adapt to long contexts and perform long-context tasks are not necessarily linked. For example, while Qwen-VL-Chat scores the highest in diagnostic evaluation among open-source models, it trails behind Mantis in task completion (39.1% &#x3C;47.5%), highlighting our evaluation’s diversity and comprehensiveness.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 8</a></span>)</span>\
**适应长上下文的能力与执行长上下文任务的能力并不一定相关。例如，尽管Qwen-VL-Chat在开源模型中诊断评估得分最高，但在任务完成率上却落后于Mantis（39.1% <47.5%），这突显了我们评估的多样性和全面性。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22E64M37FK%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B277.133%2C378.441%2C503.997%2C388.025%5D%2C%5B108%2C367.482%2C337.029%2C377.066%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%228%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=8&#x26;annotation=E64M37FK"><span style="background-color: #2ea8e580">“Interestingly, the majority of open-source models scored zero in the Image Needle in a Haystack task.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 8</a></span>)</span>\
**有趣的是，大多数开源模型在“图像大海捞针”任务中得分为零。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%229Y6X6DPA%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B108%2C321.531%2C329.376%2C330.876%5D%2C%5B108%2C310.413%2C312.436%2C319.997%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%229%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=9&#x26;annotation=9Y6X6DPA"><span style="background-color: #2ea8e580">“as the number of images increases, the performance of most models significantly declines”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 9</a></span>)</span>\
**随着图像数量的增加，大多数模型的性能显著下降。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%224KKWCUGD%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B232.493%2C255.618%2C329.376%2C265.202%5D%2C%5B108%2C244.659%2C327.72%2C254.243%5D%2C%5B108%2C233.7%2C327.72%2C243.284%5D%2C%5B108%2C222.741%2C329.376%2C232.325%5D%2C%5B108%2C211.783%2C504.001%2C221.367%5D%2C%5B108%2C200.824%2C465.392%2C210.408%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%229%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=9&#x26;annotation=4KKWCUGD"><span style="background-color: #2ea8e580">“However, the performance of GPT-4V, GPT-4o, Gemini 1.5, Claude 3 Opus and Qwen-VL-Chat on the Medium level surpasses that of the Few level. This could be attributed to their training on multi-image data, where a larger number of images can provide more information to some extent, thereby aiding the model in task completion.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 9</a></span>)</span>\
**然而，GPT-4V、GPT-4o、Gemini 1.5、Claude 3 Opus 和 Qwen-VL-Chat 在中等水平上的表现超过了在少量水平上的表现。这可能归因于它们在多图像数据上的训练，其中较多的图像在某种程度上可以提供更多的信息，从而帮助模型完成任务。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22E4CWXFTA%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B297.498%2C59.329%2C483.226%2C68.913%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%229%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=9&#x26;annotation=E4CWXFTA"><span style="background-color: #2ea8e580">“GPT-4V did not “get lost in the middle””</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 9</a></span>)</span>\
**GPT-4V 并没有“在中途迷失”。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22MWP7ZY5F%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B139.356%2C419.378%2C504.004%2C428.962%5D%2C%5B108%2C408.419%2C503.997%2C418.003%5D%2C%5B108%2C397.46%2C481.529%2C407.044%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=10&#x26;annotation=MWP7ZY5F"><span style="background-color: #2ea8e580">“On the other hand, ignoring the scenarios where the data exceeds its maximum context length (8192 tokens or 32 images) and gets truncated, Qwen-VL-Chat showed a certain degree of “lost in the middle”, particularly evident in the image needle task.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 10</a></span>)</span>\
**另一方面，忽略数据超过最大上下文长度（8192个标记或32张图片）并被截断的情况，Qwen-VL-Chat显示出一定程度的“中途丢失”，这一现象在图像针任务中尤其明显。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22A3AUAXDD%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B484.615%2C397.46%2C504.005%2C407.044%5D%2C%5B108%2C386.501%2C505.744%2C396.085%5D%2C%5B108%2C375.542%2C471.605%2C385.126%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=10&#x26;annotation=A3AUAXDD"><span style="background-color: #2ea8e580">“This indicates that the “lost in the middle” phenomenon also exists in multimodal scenarios. However, a strong ability to manage long context can significantly reduce this risk.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 10</a></span>)</span>\
**这表明“中间迷失”现象也存在于多模态场景中。然而，强大的长上下文管理能力可以显著降低这一风险。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22FZNB9RN7%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B108%2C330.013%2C296.035%2C339.597%5D%2C%5B108%2C319.055%2C296.037%2C328.639%5D%2C%5B108%2C308.096%2C175.318%2C317.68%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=10&#x26;annotation=FZNB9RN7"><span style="background-color: #2ea8e580">“Considering MILEBENCH’s use of public datasets, there’s a potential risk of data contamination.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 10</a></span>)</span>\
**考虑到MILEBENCH使用公共数据集，存在数据污染的潜在风险。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22S2U2ZZL4%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B230.711%2C253.301%2C296.041%2C262.885%5D%2C%5B107.502%2C242.342%2C297.69%2C251.926%5D%2C%5B108%2C231.383%2C296.042%2C240.967%5D%2C%5B107.701%2C220.424%2C297.687%2C230.008%5D%2C%5B108%2C209.465%2C503.997%2C219.049%5D%2C%5B107.701%2C198.507%2C504%2C208.091%5D%2C%5B108%2C187.548%2C274.027%2C197.132%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=10&#x26;annotation=S2U2ZZL4"><span style="background-color: #2ea8e580">“We referred to Wei et al. (2023) and constructed an Adversarial (ADV) Set with shuffled options and paraphrased reference answers and evaluated the difference between original and ADV results. Results (Table 3) show a negligible performance drop (0.1% ̃1.2%) for all models, indicating minimal likelihood of these models being trained on our dataset.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 10</a></span>)</span>\
**我们参考了Wei等人（2023）的研究，构建了一个包含选项打乱以及参考答案改写的对抗（ADV）集合，并评估了原始结果与ADV结果之间的差异。结果（表3）显示，所有模型的性能下降微乎其微（0.1%至1.2%），表明这些模型被训练使用我们数据集的可能性极小。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%223VIT9BEU%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B107.671%2C59.329%2C492.815%2C68.913%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=10&#x26;annotation=3VIT9BEU"><span style="background-color: #2ea8e580">“(1) The performance of proprietary models still surpasses that of open-source models”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 10</a></span>)</span>\
**专有模型的性能仍优于开源模型。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22ZJ2U3ALR%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2211%22%2C%22position%22%3A%7B%22pageIndex%22%3A10%2C%22rects%22%3A%5B%5B407.297%2C438.231%2C503.997%2C447.815%5D%2C%5B108%2C427.272%2C470.256%2C436.856%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=11&#x26;annotation=ZJ2U3ALR"><span style="background-color: #2ea8e580">“In comparison to the results on the multi-image set, the performance of proprietary models declined”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 11</a></span>)</span>\
**与多图像集上的结果相比，专有模型的性能有所下降。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22BQ3XHMME%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2211%22%2C%22position%22%3A%7B%22pageIndex%22%3A10%2C%22rects%22%3A%5B%5B304.307%2C339.601%2C503.995%2C349.185%5D%2C%5B108%2C328.642%2C466.911%2C338.226%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=11&#x26;annotation=BQ3XHMME"><span style="background-color: #2ea8e580">“Compared to the results on the multi-image set, the performance of some open-source models with short contexts improved”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 11</a></span>)</span>\
**与多图像集上的结果相比，某些具有短上下文的开源模型的性能有所提升。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22DD5F8LEV%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2211%22%2C%22position%22%3A%7B%22pageIndex%22%3A10%2C%22rects%22%3A%5B%5B174.839%2C201.795%2C327.715%2C211.379%5D%2C%5B107.671%2C190.836%2C327.719%2C200.42%5D%2C%5B108%2C179.877%2C327.717%2C189.461%5D%2C%5B108%2C168.918%2C328.963%2C178.502%5D%2C%5B108%2C157.959%2C327.714%2C167.543%5D%2C%5B108%2C147%2C203.643%2C156.584%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=11&#x26;annotation=DD5F8LEV"><span style="background-color: #2ea8e580">“We found that, aside from Task S-3 (Visual Relation Inference), tasks within the same category (either temporal multi-image or semantic multi-image) exhibited high correlation. Task S-3, being a challenging one, showed little variation in scores across models.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 11</a></span>)</span>\
**我们发现，除了任务 S-3（视觉关系推理）之外，同类别内的任务（无论是时间多图还是语义多图）表现出高度的相关性。任务 S-3 由于其难度较大，在不同模型之间的得分差异较小。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%2258U2SSBX%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%2211%22%2C%22position%22%3A%7B%22pageIndex%22%3A10%2C%22rects%22%3A%5B%5B221.64%2C147%2C327.994%2C156.584%5D%2C%5B107.691%2C136.042%2C328.711%2C145.626%5D%2C%5B108%2C125.083%2C328.959%2C134.667%5D%2C%5B107.701%2C114.124%2C327.718%2C123.708%5D%2C%5B108%2C103.165%2C329.376%2C112.749%5D%2C%5B107.701%2C92.206%2C196.302%2C101.79%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=11&#x26;annotation=58U2SSBX"><span style="background-color: #2ea8e580">“We also noted that Task T-3 (Visual Navigation and Spatial Localization) demonstrated lower correlation with other tasks, possibly due to its requirement of unique cognitive skills such as understanding the world from a firstperson perspective.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 11</a></span>)</span>\
**我们还注意到，任务T-3（视觉导航和空间定位）与其他任务的相关性较低，这可能是由于该任务需要独特的认知技能，例如从第一人称视角理解世界。**

## <span style="color: rgb(190, 27, 65)">💡论文的创新：</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22R7UTGXL7%22%2C%22color%22%3A%22%23a28ae5%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B108.249%2C235.033%2C505.655%2C244.617%5D%2C%5B108%2C224.074%2C137.916%2C233.658%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=3&#x26;annotation=R7UTGXL7"><span style="background-color: #a28ae580">“MILEBENCH consists of two major components: Realistic Evaluation and Diagnostic Evaluation”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 3</a></span>)</span>\
**MILEBENCH由两个主要部分组成：现实评估和诊断评估**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%2295T5LNLJ%22%2C%22color%22%3A%22%23a28ae5%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B107.691%2C123.338%2C505.249%2C132.922%5D%2C%5B108%2C112.379%2C347.627%2C121.963%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=3&#x26;annotation=95T5LNLJ"><span style="background-color: #a28ae580">“The realistic evaluation is designed to assess an MLLM’s ability to comprehend, integrate, and infer information in a multimodal long context.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 3</a></span>)</span>\
**多模态长上下文中的现实评估旨在评估多模态大模型（MLLM）的理解、整合和推断信息的能力。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22GZEHKQPX%22%2C%22color%22%3A%22%23a28ae5%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B354.09%2C112.379%2C503.997%2C121.963%5D%2C%5B108%2C101.42%2C457.805%2C111.004%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=3&#x26;annotation=GZEHKQPX"><span style="background-color: #a28ae580">“We categorize the tasks into two main groups: Temporal Multi-Image tasks and Semantic Multi-Image tasks.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 3</a></span>)</span>\
**我们将任务分为两大类：时间多图任务和语义多图任务。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22PVERT822%22%2C%22color%22%3A%22%23a28ae5%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B461.139%2C101.42%2C504.001%2C111.004%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=3&#x26;annotation=PVERT822"><span style="background-color: #a28ae580">“Temporal”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 3</a></span>)</span>\
**时间多图任务测试MLLM在多个时间相关图像中辨别时间关系的能力，强调模型在现实场景中的预测能力。另一方面，语义多图任务则考验MLLM处理可能与时间无关但在语义上互相关联的多个图像的能力。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22UQJPXMLV%22%2C%22color%22%3A%22%23a28ae5%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B192.753%2C670.642%2C504%2C680.226%5D%2C%5B107.502%2C659.683%2C505.247%2C669.267%5D%2C%5B107.582%2C648.724%2C504.201%2C658.308%5D%2C%5B107.611%2C636.13%2C503.996%2C647.705%5D%2C%5B108%2C625.171%2C504.004%2C634.755%5D%2C%5B108%2C614.213%2C505.747%2C623.797%5D%2C%5B107.611%2C603.254%2C504.002%2C612.838%5D%2C%5B108%2C592.295%2C493.274%2C601.879%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=7&#x26;annotation=UQJPXMLV"><span style="background-color: #a28ae580">“For the open-source dataset comprised of the benchmark, we sample 10% of the data for manual verification. Our review team, composed entirely of authors, was assigned to scrutinize the precision of the sampled data, resulting in an Inter-annotator Agreement (IAA)5 of 95%, indicating a high level of consistency among reviewers. For the datasets we formulated independently, equivalent manual verification was carried out on the entirety of the dataset, yielding a similar IAA of 98%, thus ascertaining the data quality. Additionally, the error rate was found to be less than 1% for both datasets, affirming that these datasets maintain an exceptionally high quality and are virtually devoid of errors.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 7</a></span>)</span>\
**对于包含基准的开源数据集，我们抽取了10%的数据进行人工核查。我们的审核团队由所有作者组成，负责审查抽样数据的准确性，结果表明标注者间一致性协议（Inter-annotator Agreement，IAA）达到了95%，表明审核人员之间具有较高的一致性。对于我们独立制定的数据集，我们对整个数据集进行了同等的人工核查，显示出类似的IAA为98%，从而确保数据质量。此外，两组数据集的错误率均低于1%，证明这些数据集具有极高的质量，几乎没有错误。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22YCBTDUZK%22%2C%22color%22%3A%22%23a28ae5%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B404.417%2C376.097%2C504.003%2C385.681%5D%2C%5B108%2C365.138%2C503.998%2C374.722%5D%2C%5B108%2C354.179%2C503.997%2C363.763%5D%2C%5B108%2C343.22%2C503.997%2C352.804%5D%2C%5B108%2C332.261%2C329.193%2C341.845%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=7&#x26;annotation=YCBTDUZK"><span style="background-color: #a28ae580">“When the input length exceeds the maximum context length of the model, we keep the instruction, and truncate the interleaved image-text question from left so as to keep the question of a sample, as instruction and question are critical information and the importance of the last image is higher in many tasks, e.g. multimodal dialogue.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 7</a></span>)</span>\
**当输入长度超过模型的最大上下文长度时，我们保留指令，并从左侧截断交错的图文问题，以保留样本的问题，因为指令和问题是关键信息，并且在许多任务中，例如多模态对话，最后一张图像的重要性更高。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22AJ9S7D4L%22%2C%22color%22%3A%22%23a28ae5%22%2C%22pageLabel%22%3A%2211%22%2C%22position%22%3A%7B%22pageIndex%22%3A10%2C%22rects%22%3A%5B%5B128.612%2C81.247%2C505.241%2C90.831%5D%2C%5B108%2C70.447%2C503.997%2C79.792%5D%2C%5B108%2C59.329%2C152.543%2C68.913%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=11&#x26;annotation=AJ9S7D4L"><span style="background-color: #a28ae580">“the realistic evaluation of MILEBENCH encompasses a broad range of task types, offering a more comprehensive assessment in the context of multi-image long-context scenarios.”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%2211%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 11</a></span>)</span>\
**MILEBENCH的现实评估涵盖了广泛的任务类型，在多图像长上下文场景中提供了更全面的评估。**

## <span style="color: rgb(190, 27, 65)">📗生词记录：</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22RGS3LR5I%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B352.877%2C208.126%2C400.861%2C217.71%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=1&#x26;annotation=RGS3LR5I"><span style="background-color: #aaaaaa80">“pioneering”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 1</a></span>)</span>\
**pioneering 英 \[ˌpaɪəˈnɪərɪŋ] 美 \[ˌpaɪəˈnɪrɪŋ] adj. 首创的，先驱的；有开创者特点的 v. 做先锋，倡导；开辟（道路）（pioneer 的现在分词）**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22FL2235TE%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B194.839%2C186.208%2C238.648%2C195.792%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=1&#x26;annotation=FL2235TE"><span style="background-color: #aaaaaa80">“comprises”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 1</a></span>)</span>\
**comprises v．包含，包括：由某些部分、元素或成员组成。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22GIUM7EKL%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B352.62%2C98.537%2C416.325%2C108.121%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=1&#x26;annotation=GIUM7EKL"><span style="background-color: #aaaaaa80">“intensification”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 1</a></span>)</span>\
**intensification 英 \[ɪnˌtensɪfɪˈkeɪʃ(ə)n] 美 \[ɪnˌtensɪfɪˈkeɪʃ(ə)n] n. 强化；加剧；激烈化；增强明暗度**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22MSXEL8R3%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B131.07%2C235.381%2C187.164%2C244.965%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=2&#x26;annotation=MSXEL8R3"><span style="background-color: #aaaaaa80">“encapsulates”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 2</a></span>)</span>\
**encapsulates v． 1. 概括，总结：将某事物的主要特点或要点用简洁的方式表达出来。 2. 封装：将某物包裹或封闭在另一物体内。**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22EWZQGXFN%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B402.468%2C235.381%2C444.143%2C244.965%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=2&#x26;annotation=EWZQGXFN"><span style="background-color: #aaaaaa80">“prevalent”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 2</a></span>)</span>\
**prevalent 英 \[ˈprevələnt] 美 \[ˈprevələnt] adj. 盛行的，普遍的**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22AX2HUQD4%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B332.425%2C150.943%2C390.256%2C160.527%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=2&#x26;annotation=AX2HUQD4"><span style="background-color: #aaaaaa80">“manner akin”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 2</a></span>)</span>\
**类似的方式**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22R682YQN6%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B453.352%2C507.421%2C485.542%2C517.005%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=3&#x26;annotation=R682YQN6"><span style="background-color: #aaaaaa80">“notable”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 3</a></span>)</span>\
**notable 英 \[ˈnəʊtəb(ə)l] 美 \[ˈnoʊtəb(ə)l] adj. 显要的，值得注意的；非常成功的，令人尊敬的 n. 显要人物，名流 \[ 复数 notables 比较级 more notable 最高级 most notable ]**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22MQQRQZA4%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B305.062%2C317.595%2C336.88%2C327.179%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%224%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=4&#x26;annotation=MQQRQZA4"><span style="background-color: #aaaaaa80">“discern”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 4</a></span>)</span>\
**discern 英 \[dɪˈsɜːn] 美 \[dɪˈsɜːrn] v. （艰难地或努力地）看出，觉察出；了解，认识 \[ 第三人称单数 discerns 现在分词 discerning 过去式 discerned 过去分词 discerned ]**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%22GNWYDWK9%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B117.054%2C389.4%2C173.94%2C399.013%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%228%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=8&#x26;annotation=GNWYDWK9"><span style="background-color: #aaaaaa80">“trails behind”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 8</a></span>)</span>\
**落后于**

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FDGHN9QHG%22%2C%22annotationKey%22%3A%225YUJ95AK%22%2C%22color%22%3A%22%23aaaaaa%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B122.684%2C409.043%2C171.654%2C418.627%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%229%22%7D%7D" ztype="zhighlight"><a href="zotero://open/library/items/DGHN9QHG?page=9&#x26;annotation=5YUJ95AK"><span style="background-color: #aaaaaa80">“investigate”</span></a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F17270594%2Fitems%2FSW4R2BA2%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/SW4R2BA2">Song 等, 2024, p. 9</a></span>)</span>\
**investigate 英 \[ɪnˈvestɪɡeɪt] 美 \[ɪnˈvestɪɡeɪt] v. 侦察（某事）；调查（某人）；研究 \[ 第三人称单数 investigates 现在分词 investigating 过去式 investigated 过去分词 investigated ]**

***
